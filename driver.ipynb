{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "def vgg16Model(image_size = (224, 224), if_draw_model = False):\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    \n",
    "    base_model = applications.VGG16(input_tensor=Input((height, width, 3)), weights='imagenet', include_top=False)\n",
    "\n",
    "    for layers in base_model.layers:\n",
    "        layers.trainable = True\n",
    "        \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        print(i, layer.name)\n",
    "        \n",
    "   # for layer in base_model.layers[25:]:\n",
    "   #     layer.trainable = True\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.35)(x)\n",
    "    x = Dense(10, activation='sigmoid')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    \n",
    "    if if_draw_model:\n",
    "        SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "    \n",
    "    lr = optimizers.SGD(lr=0.000085, momentum=0.9,decay=0.0, nesterov=False)\n",
    "    lr = optimizers.Adam(lr=1e-3)\n",
    "    lr = optimizers.Adadelta(lr=0.01)\n",
    "    model.compile(optimizer=lr,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "\n",
    "def restNet_model(image_size = (224, 224), if_draw_model = False):\n",
    "\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    \n",
    "    base_model = ResNet50(input_tensor=Input((height, width, 3)), weights='imagenet', include_top=False)\n",
    "\n",
    "    for layers in base_model.layers:\n",
    "        layers.trainable = True\n",
    "        \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        print(i, layer.name)\n",
    "        \n",
    "   # for layer in base_model.layers[25:]:\n",
    "   #     layer.trainable = True\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(10, activation='sigmoid')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    \n",
    "    if if_draw_model:\n",
    "        SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "    \n",
    "    lr = optimizers.Adam(lr=1e-4)\n",
    "   # lr = optimizers.SGD(lr=0.0001, momentum=0.9,decay=0.0, nesterov=False)\n",
    "    lr = optimizers.Adadelta(lr=0.1)\n",
    "    model.compile(optimizer= lr,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "def inceptionV3_model(image_size = (299, 299), if_draw_model = False):\n",
    "\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    \n",
    "    base_model = InceptionV3(input_tensor=Input((height, width, 3)), weights='imagenet', include_top=False)\n",
    "\n",
    "    for layers in base_model.layers:\n",
    "        layers.trainable = False\n",
    "        \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        print(i, layer.name)\n",
    "        \n",
    "    for layer in base_model.layers[14:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(10, activation='sigmoid')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    \n",
    "    if if_draw_model:\n",
    "        SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "    \n",
    "    lr = optimizers.SGD(lr=0.0001, momentum=0.9,decay=0.0, nesterov=False)\n",
    "    lr = optimizers.Adam(lr=5e-5)\n",
    "  #  lr = optimizers.Adadelta(lr=0.007)\n",
    "    model.compile(optimizer=lr,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "def inception_resnet_v2_model(image_size = (299, 299), if_draw_model = False):\n",
    "\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    \n",
    "    base_model = InceptionV3(input_tensor=Input((height, width, 3)), weights='imagenet', include_top=False)\n",
    "\n",
    "    for layers in base_model.layers:\n",
    "        layers.trainable = False\n",
    "        \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        print(i, layer.name)\n",
    "        \n",
    "    for layer in base_model.layers[0:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(10, activation='sigmoid')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    \n",
    "    if if_draw_model:\n",
    "        SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "    \n",
    "    lr = optimizers.SGD(lr=0.0005, momentum=0.9,decay=0.0, nesterov=False)\n",
    "  #  lr = optimizers.Adam(lr=0.1)\n",
    "    lr = optimizers.Adadelta(lr=0.1)\n",
    "    model.compile(optimizer=lr,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单模型\n",
    "def simple_model(time_len=1):\n",
    "    ch, row, col = 3, 224, 224  # camera format\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "  #  model.add(Lambda(lambda x: x,\n",
    "            input_shape=( row, col, ch),\n",
    "            output_shape=( row, col,ch)))\n",
    "    model.add(Convolution2D(3, 3, 3, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(10))\n",
    "#  model.add(Lambda(nor_output_1))\n",
    "    sgd = optimizers.SGD(lr=0.00003, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "      \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(2017)\n",
    "\n",
    "def get_driver_list():\n",
    "    driver_id_list = []\n",
    "    df = pd.read_csv(\"driver_imgs_list.csv\")\n",
    "    for i in range(df.shape[0]):\n",
    "        driver_id = df.loc[i][\"subject\"]\n",
    "        is_saved_id = False\n",
    "        for saved_id in driver_id_list:\n",
    "            if saved_id == driver_id:\n",
    "                is_saved_id = True\n",
    "                break\n",
    "        \n",
    "        if is_saved_id == False:\n",
    "            driver_id_list.append(driver_id)\n",
    "    return driver_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data():\n",
    "    dirbase = 'imgs/train2/'\n",
    "    if os.path.exists(dirbase):\n",
    "        return\n",
    "    os.mkdir(dirbase)\n",
    "        \n",
    "    driver_id_list = get_driver_list()\n",
    "    \n",
    "    df = pd.read_csv(\"driver_imgs_list.csv\")  \n",
    "    sub_dirs = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "    for driver_id in driver_id_list:\n",
    "        os.mkdir(dirbase + driver_id)\n",
    "        for sub_dir in sub_dirs:\n",
    "            os.mkdir(dirbase + driver_id + '/' + sub_dir)\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        driver_id = df.loc[i][\"subject\"]\n",
    "        class_name = df.loc[i][\"classname\"]\n",
    "        img = df.loc[i][\"img\"]\n",
    "        add_old = 'imgs/train/' + class_name+'/' + img\n",
    "        add_new = dirbase + driver_id + '/' + class_name + '/' + img\n",
    "        #if os.path.exists(add_old):\n",
    "        shutil.move(add_old, add_new)\n",
    "        \n",
    "    os.mkdir('imgs/data')\n",
    "    shutil.move('imgs/test', 'imgs/data/')\n",
    "        \n",
    "prepare_data() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=(299, 299)\n",
    "batch_size =64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "from keras.callbacks import *\n",
    "from keras.models import *\n",
    "#from keras.applications.vgg19 import preprocess_input\n",
    "#from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "\n",
    "import copy\n",
    "def get_file_num(driver_id):\n",
    "    train_base_add = 'imgs/train2/'\n",
    "    add_driver = train_base_add + driver_id\n",
    "    sub_dirs = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "    num = 0\n",
    "    for sub_dir in sub_dirs:\n",
    "        add_driver_class = add_driver + '/' + sub_dir\n",
    "        train_filenames = os.listdir(add_driver_class)\n",
    "        num += len(train_filenames)\n",
    "    return num\n",
    "\n",
    "def train_model(Model, save_file='model.h5'):\n",
    "    model = Model()\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(\n",
    " #   shear_range=0.2,\n",
    " #   zoom_range=0.2,\n",
    "    preprocessing_function=preprocess_input,\n",
    " #   horizontal_flip=True\n",
    "    )\n",
    "    driver_id_list = get_driver_list()\n",
    "    train_base_add = 'imgs/train2/'\n",
    "    \n",
    "    \n",
    "    patience = 4\n",
    "    loss_list = []\n",
    "    \n",
    "    best_model = 0\n",
    "    save_loss_callback = LambdaCallback(on_epoch_end=lambda epoch, logs:loss_list.append(logs['val_loss']))\n",
    "                            \n",
    "    for epoch in range(25):\n",
    "        print('step---------------------- {%d}'%(epoch))\n",
    "        for driver_id in driver_id_list:\n",
    "            nb_train_samples = get_file_num(driver_id)\n",
    "            \n",
    "            train_generator = train_datagen.flow_from_directory(\n",
    "                train_base_add + driver_id,\n",
    "                target_size=(image_size[0], image_size[1]),\n",
    "                batch_size=batch_size,\n",
    "                class_mode='categorical')\n",
    "    \n",
    "            val_generator = train_datagen.flow_from_directory(\n",
    "                'imgs/val',\n",
    "                target_size=(image_size[0], image_size[1]),\n",
    "                batch_size=batch_size,\n",
    "                class_mode='categorical')\n",
    "        \n",
    "            model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples // batch_size,\n",
    "                epochs=1,\n",
    "                workers=6,\n",
    "                callbacks=[save_loss_callback],\n",
    "                validation_data = val_generator)\n",
    "            \n",
    "            if epoch >= 0:\n",
    "                last_loss = loss_list[-1]\n",
    "                if_best_loss = True\n",
    "                for loss in loss_list:\n",
    "                    if loss < last_loss:\n",
    "                        if_best_loss = False\n",
    "                if if_best_loss == True:\n",
    "                #    best_model =copy.deepcopy(model) \n",
    "                #    if epoch >= 6:\n",
    "                    print('last model loss ------------------------------------------%f'%(last_loss))\n",
    "                    model.save(save_file)\n",
    "    \n",
    "    \n",
    "    model = load_model(save_file)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "from keras.models import *\n",
    "#from keras.applications.vgg16 import preprocess_input\n",
    "#from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input, decode_predictions\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from skimage.io import imread, imsave\n",
    "from scipy.misc import imresize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def get_file_num(driver_id):\n",
    "    train_base_add = 'imgs/train2/'\n",
    "    add_driver = train_base_add + driver_id\n",
    "    sub_dirs = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "    num = 0\n",
    "    for sub_dir in sub_dirs:\n",
    "        add_driver_class = add_driver + '/' + sub_dir\n",
    "        train_filenames = os.listdir(add_driver_class)\n",
    "        num += len(train_filenames)\n",
    "    return num\n",
    "\n",
    "def load_image(path):\n",
    "    img = imread(path)\n",
    "    img = imresize(img, (image_size[1], image_size[0]))\n",
    "    return img\n",
    "\n",
    "def load_driver_imgs(driver_id, pre_add = 'imgs/train2/'):\n",
    "    base_add = pre_add + driver_id\n",
    "    sub_dirs = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "    \n",
    "    img_list = []\n",
    "    label_list = []\n",
    "    for i, sub_dir in enumerate(sub_dirs) :\n",
    "        img_class_add = base_add + '/' + sub_dir\n",
    "        img_names = os.listdir(img_class_add)\n",
    "        for img_name in img_names:\n",
    "            \n",
    "            img = load_image(img_class_add + '/' + img_name)\n",
    "            img_list.append(img)\n",
    "        \n",
    "            label_list.append(sub_dir)\n",
    "            \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(label_list)\n",
    "    encoded_Y = encoder.transform(label_list)\n",
    "    encoded_Y.astype(np.float32)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "    label_list = np_utils.to_categorical(encoded_Y)\n",
    " \n",
    "  #  img_list = np.array(img_list)\n",
    "  #  label_list = np.array(label_list)\n",
    "    return img_list, label_list\n",
    "\n",
    "def train_model_2(Model, save_file='model.h5'):\n",
    "    model = Model()\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(\n",
    " #   shear_range=0.2,\n",
    " #   zoom_range=0.2,\n",
    "    preprocessing_function=preprocess_input,\n",
    " #   horizontal_flip=True\n",
    "    )\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(\n",
    " #   shear_range=0.2,\n",
    " #   zoom_range=0.2,\n",
    "    preprocessing_function=preprocess_input,\n",
    " #   horizontal_flip=True\n",
    "    )\n",
    "    \n",
    "    driver_id_list = get_driver_list()\n",
    "    train_base_add = 'imgs/train2/'\n",
    "    \n",
    "    \n",
    "    patience = 5\n",
    "    loss_list = []\n",
    "    \n",
    "    val_driver_list = driver_id_list[0:4]\n",
    "    train_driver_list = driver_id_list[4:]\n",
    "   \n",
    "    x_val = []\n",
    "    y_val = []\n",
    "    for driver_id in val_driver_list:\n",
    "        print(driver_id)\n",
    "        x, y = load_driver_imgs(driver_id)\n",
    "        x_val.extend(x)\n",
    "        y_val.extend(y)\n",
    "    x_val = np.array(x_val)\n",
    "    y_val = np.array(y_val)\n",
    "    val_datagen.fit(x_val)\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for driver_id in train_driver_list:\n",
    "        nb_train_samples = get_file_num(driver_id)\n",
    "            \n",
    "        x, y = load_driver_imgs(driver_id)\n",
    "        x_train.extend(x)\n",
    "        y_train.extend(y)\n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    train_datagen.fit(x_train)\n",
    "    \n",
    "    model.fit_generator(\n",
    "                    train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    shuffle=True,\n",
    "                    steps_per_epoch=len(x_train) // batch_size,\n",
    "                    epochs=60,\n",
    "                    workers=6,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=4, verbose=0)],\n",
    "                    validation_data = val_datagen.flow(x_val, y_val, batch_size=batch_size)               \n",
    "                )\n",
    " #   model = load_model('model.h5')\n",
    "    model.save(save_file)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "def test_model(model, file_name):\n",
    "    df = pd.read_csv(\"sample_submission.csv\")\n",
    "    \n",
    "    test_address = 'imgs_2/data/test'\n",
    "    test_filenames = os.listdir(test_address)\n",
    "  #  test_filenames = test_filenames[:10]\n",
    "    X = np.zeros((len(test_filenames), image_size[0], image_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "    for i, test_file in enumerate(test_filenames) :\n",
    "        X[i] = cv2.resize(cv2.imread(test_address + '/' + test_file), image_size)\n",
    "    \n",
    "    y_pred = model.predict(X, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    \n",
    "    for i, fname in enumerate(test_filenames):\n",
    "      #  print(fname)\n",
    "        df.set_value(i, 'img', fname)\n",
    "        sub_dirs = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "        for ii, sb in enumerate(sub_dirs):\n",
    "            df.set_value(i, sb, y_pred[i][ii])\n",
    "\n",
    "\n",
    "    df.to_csv(file_name, index=None)\n",
    "    df.head(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "#from keras.applications.vgg16 import preprocess_input\n",
    "#from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "#from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input, decode_predictions\n",
    "\n",
    "def test_model_2(model, file_name):\n",
    "    df = pd.read_csv(\"sample_submission.csv\")\n",
    "    \n",
    "    test_address = 'imgs/data'\n",
    "\n",
    "  #  test_address = 'imgs_2/data/test'\n",
    "    test_filenames = os.listdir(test_address + '/test')\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    " #   shear_range=0.2,\n",
    " #   zoom_range=0.2,\n",
    "    preprocessing_function=preprocess_input,\n",
    " #   horizontal_flip=True\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_address,\n",
    "        target_size=(image_size[0], image_size[1]),\n",
    "        shuffle = False,\n",
    "        class_mode='categorical',\n",
    "        batch_size=1)\n",
    "\n",
    "    filenames = test_generator.filenames\n",
    "    nb_samples = len(filenames)\n",
    "   \n",
    "    print(len(test_filenames))\n",
    "    y_pred = model.predict_generator(test_generator, steps = len(test_filenames))\n",
    "  #  y_pred = y_pred.clip(min=0.005, max=0.995)\n",
    "    print(y_pred)\n",
    "    print(y_pred.shape)\n",
    "    for i, fname in enumerate(filenames):\n",
    "     #   print(fname)\n",
    "        fname_con_list = fname.split('/')\n",
    "        df.set_value(i, 'img', fname_con_list[-1])\n",
    "        sub_dirs = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "        for ii, sb in enumerate(sub_dirs):\n",
    "            df.set_value(i, sb, y_pred[i][ii])\n",
    "\n",
    "\n",
    "    df.to_csv(file_name, index=None)\n",
    "    df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (3, 3), strides=(2, 2), padding=\"same\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dlnd-tf-lab/lib/python3.5/site-packages/ipykernel_launcher.py:25: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p012\n",
      "p014\n",
      "p015\n"
     ]
    }
   ],
   "source": [
    "model = train_model_2(simple_model, 'model.h5')\n",
    "test_model_2(model, 'pred.csv')\n",
    "data = pd.read_csv(\"pred.csv\")\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_pool\n",
      "11 block4_conv1\n",
      "12 block4_conv2\n",
      "13 block4_conv3\n",
      "14 block4_pool\n",
      "15 block5_conv1\n",
      "16 block5_conv2\n",
      "17 block5_conv3\n",
      "18 block5_pool\n",
      "p002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:25: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p012\n",
      "p014\n",
      "p015\n",
      "Epoch 1/60\n",
      "298/298 [==============================] - 337s 1s/step - loss: 2.2850 - acc: 0.1360 - val_loss: 2.1562 - val_acc: 0.1852\n",
      "Epoch 2/60\n",
      "298/298 [==============================] - 132s 444ms/step - loss: 1.5027 - acc: 0.4714 - val_loss: 1.2685 - val_acc: 0.6072\n",
      "Epoch 3/60\n",
      "298/298 [==============================] - 132s 444ms/step - loss: 0.5340 - acc: 0.8370 - val_loss: 0.7828 - val_acc: 0.7660\n",
      "Epoch 4/60\n",
      "298/298 [==============================] - 132s 444ms/step - loss: 0.2375 - acc: 0.9325 - val_loss: 0.7801 - val_acc: 0.7751\n",
      "Epoch 5/60\n",
      "298/298 [==============================] - 133s 445ms/step - loss: 0.1479 - acc: 0.9603 - val_loss: 0.6339 - val_acc: 0.8175\n",
      "Epoch 6/60\n",
      "298/298 [==============================] - 133s 445ms/step - loss: 0.1052 - acc: 0.9708 - val_loss: 0.7927 - val_acc: 0.7824\n",
      "Epoch 7/60\n",
      "298/298 [==============================] - 133s 445ms/step - loss: 0.0771 - acc: 0.9792 - val_loss: 0.7347 - val_acc: 0.8105\n",
      "Epoch 8/60\n",
      "298/298 [==============================] - 132s 444ms/step - loss: 0.0663 - acc: 0.9826 - val_loss: 0.8141 - val_acc: 0.7824\n",
      "Epoch 9/60\n",
      "298/298 [==============================] - 132s 444ms/step - loss: 0.0534 - acc: 0.9864 - val_loss: 0.7322 - val_acc: 0.8221\n",
      "Epoch 10/60\n",
      "298/298 [==============================] - 132s 444ms/step - loss: 0.0471 - acc: 0.9873 - val_loss: 0.7024 - val_acc: 0.8296\n",
      "Epoch 11/60\n",
      "298/298 [==============================] - 132s 444ms/step - loss: 0.0340 - acc: 0.9904 - val_loss: 0.8613 - val_acc: 0.7802\n",
      "Found 79726 images belonging to 1 classes.\n",
      "79726\n",
      "[[  1.92442053e-15   6.12534205e-16   1.71000136e-17 ...,   1.89627002e-15\n",
      "    1.53955348e-10   1.94814087e-12]\n",
      " [  4.29588240e-11   3.66705852e-13   5.05686462e-15 ...,   1.13028237e-14\n",
      "    5.61738800e-14   1.77188071e-13]\n",
      " [  6.30361470e-08   1.03775233e-09   5.26507345e-14 ...,   1.00059344e-11\n",
      "    2.38593919e-11   7.32391703e-09]\n",
      " ..., \n",
      " [  4.00903833e-12   1.22052660e-10   6.68858068e-12 ...,   7.88114733e-07\n",
      "    3.60066998e-08   8.62661831e-10]\n",
      " [  2.60839608e-13   1.53347751e-11   3.57126630e-08 ...,   2.51310472e-10\n",
      "    8.17056023e-09   3.06027117e-15]\n",
      " [  3.20194343e-10   2.21708177e-12   1.14671137e-11 ...,   6.71097622e-10\n",
      "    6.84296925e-11   4.28437737e-11]]\n",
      "(79726, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:39: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:42: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_1.jpg</td>\n",
       "      <td>1.924421e-15</td>\n",
       "      <td>6.125342e-16</td>\n",
       "      <td>1.710001e-17</td>\n",
       "      <td>1.697531e-15</td>\n",
       "      <td>1.191249e-16</td>\n",
       "      <td>3.789859e-04</td>\n",
       "      <td>1.003671e-18</td>\n",
       "      <td>1.896270e-15</td>\n",
       "      <td>1.539553e-10</td>\n",
       "      <td>1.948141e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_10.jpg</td>\n",
       "      <td>4.295882e-11</td>\n",
       "      <td>3.667059e-13</td>\n",
       "      <td>5.056865e-15</td>\n",
       "      <td>3.184777e-15</td>\n",
       "      <td>1.309251e-15</td>\n",
       "      <td>2.812556e-04</td>\n",
       "      <td>5.283374e-17</td>\n",
       "      <td>1.130282e-14</td>\n",
       "      <td>5.617388e-14</td>\n",
       "      <td>1.771881e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_100.jpg</td>\n",
       "      <td>6.303615e-08</td>\n",
       "      <td>1.037752e-09</td>\n",
       "      <td>5.265073e-14</td>\n",
       "      <td>1.639867e-12</td>\n",
       "      <td>3.025315e-12</td>\n",
       "      <td>5.463375e-11</td>\n",
       "      <td>1.706329e-14</td>\n",
       "      <td>1.000593e-11</td>\n",
       "      <td>2.385939e-11</td>\n",
       "      <td>7.323917e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_1000.jpg</td>\n",
       "      <td>3.337894e-14</td>\n",
       "      <td>1.182375e-13</td>\n",
       "      <td>2.683266e-06</td>\n",
       "      <td>1.154419e-14</td>\n",
       "      <td>3.252749e-15</td>\n",
       "      <td>6.357867e-12</td>\n",
       "      <td>3.802739e-12</td>\n",
       "      <td>4.007042e-16</td>\n",
       "      <td>4.727758e-02</td>\n",
       "      <td>3.481190e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_100000.jpg</td>\n",
       "      <td>7.673723e-11</td>\n",
       "      <td>1.541353e-09</td>\n",
       "      <td>2.174207e-16</td>\n",
       "      <td>2.064765e-02</td>\n",
       "      <td>6.863557e-11</td>\n",
       "      <td>3.999730e-11</td>\n",
       "      <td>1.447392e-13</td>\n",
       "      <td>5.463732e-14</td>\n",
       "      <td>3.052532e-10</td>\n",
       "      <td>9.723730e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              img            c0            c1            c2            c3  \\\n",
       "0       img_1.jpg  1.924421e-15  6.125342e-16  1.710001e-17  1.697531e-15   \n",
       "1      img_10.jpg  4.295882e-11  3.667059e-13  5.056865e-15  3.184777e-15   \n",
       "2     img_100.jpg  6.303615e-08  1.037752e-09  5.265073e-14  1.639867e-12   \n",
       "3    img_1000.jpg  3.337894e-14  1.182375e-13  2.683266e-06  1.154419e-14   \n",
       "4  img_100000.jpg  7.673723e-11  1.541353e-09  2.174207e-16  2.064765e-02   \n",
       "\n",
       "             c4            c5            c6            c7            c8  \\\n",
       "0  1.191249e-16  3.789859e-04  1.003671e-18  1.896270e-15  1.539553e-10   \n",
       "1  1.309251e-15  2.812556e-04  5.283374e-17  1.130282e-14  5.617388e-14   \n",
       "2  3.025315e-12  5.463375e-11  1.706329e-14  1.000593e-11  2.385939e-11   \n",
       "3  3.252749e-15  6.357867e-12  3.802739e-12  4.007042e-16  4.727758e-02   \n",
       "4  6.863557e-11  3.999730e-11  1.447392e-13  5.463732e-14  3.052532e-10   \n",
       "\n",
       "             c9  \n",
       "0  1.948141e-12  \n",
       "1  1.771881e-13  \n",
       "2  7.323917e-09  \n",
       "3  3.481190e-10  \n",
       "4  9.723730e-10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train_model_2(vgg16Model, save_file='model_vgg.h5')\n",
    "test_model_2(model, 'pred.csv')\n",
    "data = pd.read_csv(\"pred.csv\")\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_7\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_pool\n",
      "11 block4_conv1\n",
      "12 block4_conv2\n",
      "13 block4_conv3\n",
      "14 block4_pool\n",
      "15 block5_conv1\n",
      "16 block5_conv2\n",
      "17 block5_conv3\n",
      "18 block5_pool\n",
      "p002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:25: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p012\n",
      "p014\n",
      "p015\n",
      "Epoch 1/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 2.4099 - acc: 0.1379 - val_loss: 2.2829 - val_acc: 0.1158\n",
      "Epoch 2/60\n",
      "298/298 [==============================] - 134s 449ms/step - loss: 1.7246 - acc: 0.3911 - val_loss: 1.6761 - val_acc: 0.4329\n",
      "Epoch 3/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.7855 - acc: 0.7648 - val_loss: 1.2049 - val_acc: 0.6084\n",
      "Epoch 4/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.4179 - acc: 0.8814 - val_loss: 1.0713 - val_acc: 0.6560\n",
      "Epoch 5/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.2759 - acc: 0.9252 - val_loss: 1.0243 - val_acc: 0.6905\n",
      "Epoch 6/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.2151 - acc: 0.9420 - val_loss: 0.9809 - val_acc: 0.7087\n",
      "Epoch 7/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.1585 - acc: 0.9575 - val_loss: 1.1021 - val_acc: 0.7181\n",
      "Epoch 8/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.1356 - acc: 0.9638 - val_loss: 0.8684 - val_acc: 0.7602\n",
      "Epoch 9/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.1196 - acc: 0.9697 - val_loss: 0.9910 - val_acc: 0.7278\n",
      "Epoch 10/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.0975 - acc: 0.9755 - val_loss: 1.0250 - val_acc: 0.7457\n",
      "Epoch 11/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.0896 - acc: 0.9778 - val_loss: 0.9912 - val_acc: 0.7505\n",
      "Epoch 12/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.0794 - acc: 0.9804 - val_loss: 1.0520 - val_acc: 0.7372\n",
      "Epoch 13/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.0654 - acc: 0.9841 - val_loss: 1.0521 - val_acc: 0.7448\n",
      "Epoch 14/60\n",
      "298/298 [==============================] - 134s 450ms/step - loss: 0.0636 - acc: 0.9840 - val_loss: 1.0701 - val_acc: 0.7381\n",
      "Found 79726 images belonging to 1 classes.\n",
      "79726\n"
     ]
    }
   ],
   "source": [
    "#lr adam 0.000085,dr 0.5\n",
    "model = train_model_2(vgg16Model, save_file='model_vgg.h5')\n",
    "test_model_2(model, 'pred.csv')\n",
    "data = pd.read_csv(\"pred.csv\")\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 1s 0us/step\n",
      "0 input_1\n",
      "1 conv1\n",
      "2 bn_conv1\n",
      "3 activation_1\n",
      "4 max_pooling2d_1\n",
      "5 res2a_branch2a\n",
      "6 bn2a_branch2a\n",
      "7 activation_2\n",
      "8 res2a_branch2b\n",
      "9 bn2a_branch2b\n",
      "10 activation_3\n",
      "11 res2a_branch2c\n",
      "12 res2a_branch1\n",
      "13 bn2a_branch2c\n",
      "14 bn2a_branch1\n",
      "15 add_1\n",
      "16 activation_4\n",
      "17 res2b_branch2a\n",
      "18 bn2b_branch2a\n",
      "19 activation_5\n",
      "20 res2b_branch2b\n",
      "21 bn2b_branch2b\n",
      "22 activation_6\n",
      "23 res2b_branch2c\n",
      "24 bn2b_branch2c\n",
      "25 add_2\n",
      "26 activation_7\n",
      "27 res2c_branch2a\n",
      "28 bn2c_branch2a\n",
      "29 activation_8\n",
      "30 res2c_branch2b\n",
      "31 bn2c_branch2b\n",
      "32 activation_9\n",
      "33 res2c_branch2c\n",
      "34 bn2c_branch2c\n",
      "35 add_3\n",
      "36 activation_10\n",
      "37 res3a_branch2a\n",
      "38 bn3a_branch2a\n",
      "39 activation_11\n",
      "40 res3a_branch2b\n",
      "41 bn3a_branch2b\n",
      "42 activation_12\n",
      "43 res3a_branch2c\n",
      "44 res3a_branch1\n",
      "45 bn3a_branch2c\n",
      "46 bn3a_branch1\n",
      "47 add_4\n",
      "48 activation_13\n",
      "49 res3b_branch2a\n",
      "50 bn3b_branch2a\n",
      "51 activation_14\n",
      "52 res3b_branch2b\n",
      "53 bn3b_branch2b\n",
      "54 activation_15\n",
      "55 res3b_branch2c\n",
      "56 bn3b_branch2c\n",
      "57 add_5\n",
      "58 activation_16\n",
      "59 res3c_branch2a\n",
      "60 bn3c_branch2a\n",
      "61 activation_17\n",
      "62 res3c_branch2b\n",
      "63 bn3c_branch2b\n",
      "64 activation_18\n",
      "65 res3c_branch2c\n",
      "66 bn3c_branch2c\n",
      "67 add_6\n",
      "68 activation_19\n",
      "69 res3d_branch2a\n",
      "70 bn3d_branch2a\n",
      "71 activation_20\n",
      "72 res3d_branch2b\n",
      "73 bn3d_branch2b\n",
      "74 activation_21\n",
      "75 res3d_branch2c\n",
      "76 bn3d_branch2c\n",
      "77 add_7\n",
      "78 activation_22\n",
      "79 res4a_branch2a\n",
      "80 bn4a_branch2a\n",
      "81 activation_23\n",
      "82 res4a_branch2b\n",
      "83 bn4a_branch2b\n",
      "84 activation_24\n",
      "85 res4a_branch2c\n",
      "86 res4a_branch1\n",
      "87 bn4a_branch2c\n",
      "88 bn4a_branch1\n",
      "89 add_8\n",
      "90 activation_25\n",
      "91 res4b_branch2a\n",
      "92 bn4b_branch2a\n",
      "93 activation_26\n",
      "94 res4b_branch2b\n",
      "95 bn4b_branch2b\n",
      "96 activation_27\n",
      "97 res4b_branch2c\n",
      "98 bn4b_branch2c\n",
      "99 add_9\n",
      "100 activation_28\n",
      "101 res4c_branch2a\n",
      "102 bn4c_branch2a\n",
      "103 activation_29\n",
      "104 res4c_branch2b\n",
      "105 bn4c_branch2b\n",
      "106 activation_30\n",
      "107 res4c_branch2c\n",
      "108 bn4c_branch2c\n",
      "109 add_10\n",
      "110 activation_31\n",
      "111 res4d_branch2a\n",
      "112 bn4d_branch2a\n",
      "113 activation_32\n",
      "114 res4d_branch2b\n",
      "115 bn4d_branch2b\n",
      "116 activation_33\n",
      "117 res4d_branch2c\n",
      "118 bn4d_branch2c\n",
      "119 add_11\n",
      "120 activation_34\n",
      "121 res4e_branch2a\n",
      "122 bn4e_branch2a\n",
      "123 activation_35\n",
      "124 res4e_branch2b\n",
      "125 bn4e_branch2b\n",
      "126 activation_36\n",
      "127 res4e_branch2c\n",
      "128 bn4e_branch2c\n",
      "129 add_12\n",
      "130 activation_37\n",
      "131 res4f_branch2a\n",
      "132 bn4f_branch2a\n",
      "133 activation_38\n",
      "134 res4f_branch2b\n",
      "135 bn4f_branch2b\n",
      "136 activation_39\n",
      "137 res4f_branch2c\n",
      "138 bn4f_branch2c\n",
      "139 add_13\n",
      "140 activation_40\n",
      "141 res5a_branch2a\n",
      "142 bn5a_branch2a\n",
      "143 activation_41\n",
      "144 res5a_branch2b\n",
      "145 bn5a_branch2b\n",
      "146 activation_42\n",
      "147 res5a_branch2c\n",
      "148 res5a_branch1\n",
      "149 bn5a_branch2c\n",
      "150 bn5a_branch1\n",
      "151 add_14\n",
      "152 activation_43\n",
      "153 res5b_branch2a\n",
      "154 bn5b_branch2a\n",
      "155 activation_44\n",
      "156 res5b_branch2b\n",
      "157 bn5b_branch2b\n",
      "158 activation_45\n",
      "159 res5b_branch2c\n",
      "160 bn5b_branch2c\n",
      "161 add_15\n",
      "162 activation_46\n",
      "163 res5c_branch2a\n",
      "164 bn5c_branch2a\n",
      "165 activation_47\n",
      "166 res5c_branch2b\n",
      "167 bn5c_branch2b\n",
      "168 activation_48\n",
      "169 res5c_branch2c\n",
      "170 bn5c_branch2c\n",
      "171 add_16\n",
      "172 activation_49\n",
      "173 avg_pool\n",
      "p002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:25: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p012\n",
      "p014\n",
      "p015\n",
      "Epoch 1/60\n",
      "298/298 [==============================] - 306s 1s/step - loss: 0.2762 - acc: 0.9331 - val_loss: 0.5556 - val_acc: 0.8427\n",
      "Epoch 2/60\n",
      "298/298 [==============================] - 104s 348ms/step - loss: 0.0094 - acc: 0.9983 - val_loss: 0.7615 - val_acc: 0.7981\n",
      "Epoch 3/60\n",
      "298/298 [==============================] - 104s 348ms/step - loss: 0.0117 - acc: 0.9974 - val_loss: 0.8385 - val_acc: 0.7711\n",
      "Epoch 4/60\n",
      "298/298 [==============================] - 104s 349ms/step - loss: 0.0187 - acc: 0.9944 - val_loss: 0.8355 - val_acc: 0.7523\n",
      "Epoch 5/60\n",
      "298/298 [==============================] - 104s 348ms/step - loss: 0.0034 - acc: 0.9993 - val_loss: 0.8843 - val_acc: 0.7551\n",
      "Epoch 6/60\n",
      "298/298 [==============================] - 104s 348ms/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.7040 - val_acc: 0.7960\n",
      "Epoch 7/60\n",
      "110/298 [==========>...................] - ETA: 1:01 - loss: 9.3277e-04 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "#lr adam 0.0001,dr 0.35\n",
    "model = train_model_2(restNet_model, save_file='model_resnet.h5')\n",
    "test_model_2(model, 'pred.csv')\n",
    "data = pd.read_csv(\"pred.csv\")\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 conv1\n",
      "2 bn_conv1\n",
      "3 activation_50\n",
      "4 max_pooling2d_2\n",
      "5 res2a_branch2a\n",
      "6 bn2a_branch2a\n",
      "7 activation_51\n",
      "8 res2a_branch2b\n",
      "9 bn2a_branch2b\n",
      "10 activation_52\n",
      "11 res2a_branch2c\n",
      "12 res2a_branch1\n",
      "13 bn2a_branch2c\n",
      "14 bn2a_branch1\n",
      "15 add_17\n",
      "16 activation_53\n",
      "17 res2b_branch2a\n",
      "18 bn2b_branch2a\n",
      "19 activation_54\n",
      "20 res2b_branch2b\n",
      "21 bn2b_branch2b\n",
      "22 activation_55\n",
      "23 res2b_branch2c\n",
      "24 bn2b_branch2c\n",
      "25 add_18\n",
      "26 activation_56\n",
      "27 res2c_branch2a\n",
      "28 bn2c_branch2a\n",
      "29 activation_57\n",
      "30 res2c_branch2b\n",
      "31 bn2c_branch2b\n",
      "32 activation_58\n",
      "33 res2c_branch2c\n",
      "34 bn2c_branch2c\n",
      "35 add_19\n",
      "36 activation_59\n",
      "37 res3a_branch2a\n",
      "38 bn3a_branch2a\n",
      "39 activation_60\n",
      "40 res3a_branch2b\n",
      "41 bn3a_branch2b\n",
      "42 activation_61\n",
      "43 res3a_branch2c\n",
      "44 res3a_branch1\n",
      "45 bn3a_branch2c\n",
      "46 bn3a_branch1\n",
      "47 add_20\n",
      "48 activation_62\n",
      "49 res3b_branch2a\n",
      "50 bn3b_branch2a\n",
      "51 activation_63\n",
      "52 res3b_branch2b\n",
      "53 bn3b_branch2b\n",
      "54 activation_64\n",
      "55 res3b_branch2c\n",
      "56 bn3b_branch2c\n",
      "57 add_21\n",
      "58 activation_65\n",
      "59 res3c_branch2a\n",
      "60 bn3c_branch2a\n",
      "61 activation_66\n",
      "62 res3c_branch2b\n",
      "63 bn3c_branch2b\n",
      "64 activation_67\n",
      "65 res3c_branch2c\n",
      "66 bn3c_branch2c\n",
      "67 add_22\n",
      "68 activation_68\n",
      "69 res3d_branch2a\n",
      "70 bn3d_branch2a\n",
      "71 activation_69\n",
      "72 res3d_branch2b\n",
      "73 bn3d_branch2b\n",
      "74 activation_70\n",
      "75 res3d_branch2c\n",
      "76 bn3d_branch2c\n",
      "77 add_23\n",
      "78 activation_71\n",
      "79 res4a_branch2a\n",
      "80 bn4a_branch2a\n",
      "81 activation_72\n",
      "82 res4a_branch2b\n",
      "83 bn4a_branch2b\n",
      "84 activation_73\n",
      "85 res4a_branch2c\n",
      "86 res4a_branch1\n",
      "87 bn4a_branch2c\n",
      "88 bn4a_branch1\n",
      "89 add_24\n",
      "90 activation_74\n",
      "91 res4b_branch2a\n",
      "92 bn4b_branch2a\n",
      "93 activation_75\n",
      "94 res4b_branch2b\n",
      "95 bn4b_branch2b\n",
      "96 activation_76\n",
      "97 res4b_branch2c\n",
      "98 bn4b_branch2c\n",
      "99 add_25\n",
      "100 activation_77\n",
      "101 res4c_branch2a\n",
      "102 bn4c_branch2a\n",
      "103 activation_78\n",
      "104 res4c_branch2b\n",
      "105 bn4c_branch2b\n",
      "106 activation_79\n",
      "107 res4c_branch2c\n",
      "108 bn4c_branch2c\n",
      "109 add_26\n",
      "110 activation_80\n",
      "111 res4d_branch2a\n",
      "112 bn4d_branch2a\n",
      "113 activation_81\n",
      "114 res4d_branch2b\n",
      "115 bn4d_branch2b\n",
      "116 activation_82\n",
      "117 res4d_branch2c\n",
      "118 bn4d_branch2c\n",
      "119 add_27\n",
      "120 activation_83\n",
      "121 res4e_branch2a\n",
      "122 bn4e_branch2a\n",
      "123 activation_84\n",
      "124 res4e_branch2b\n",
      "125 bn4e_branch2b\n",
      "126 activation_85\n",
      "127 res4e_branch2c\n",
      "128 bn4e_branch2c\n",
      "129 add_28\n",
      "130 activation_86\n",
      "131 res4f_branch2a\n",
      "132 bn4f_branch2a\n",
      "133 activation_87\n",
      "134 res4f_branch2b\n",
      "135 bn4f_branch2b\n",
      "136 activation_88\n",
      "137 res4f_branch2c\n",
      "138 bn4f_branch2c\n",
      "139 add_29\n",
      "140 activation_89\n",
      "141 res5a_branch2a\n",
      "142 bn5a_branch2a\n",
      "143 activation_90\n",
      "144 res5a_branch2b\n",
      "145 bn5a_branch2b\n",
      "146 activation_91\n",
      "147 res5a_branch2c\n",
      "148 res5a_branch1\n",
      "149 bn5a_branch2c\n",
      "150 bn5a_branch1\n",
      "151 add_30\n",
      "152 activation_92\n",
      "153 res5b_branch2a\n",
      "154 bn5b_branch2a\n",
      "155 activation_93\n",
      "156 res5b_branch2b\n",
      "157 bn5b_branch2b\n",
      "158 activation_94\n",
      "159 res5b_branch2c\n",
      "160 bn5b_branch2c\n",
      "161 add_31\n",
      "162 activation_95\n",
      "163 res5c_branch2a\n",
      "164 bn5c_branch2a\n",
      "165 activation_96\n",
      "166 res5c_branch2b\n",
      "167 bn5c_branch2b\n",
      "168 activation_97\n",
      "169 res5c_branch2c\n",
      "170 bn5c_branch2c\n",
      "171 add_32\n",
      "172 activation_98\n",
      "173 avg_pool\n",
      "p002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:25: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p012\n",
      "p014\n",
      "p015\n",
      "Epoch 1/60\n",
      "298/298 [==============================] - 109s 365ms/step - loss: 0.2441 - acc: 0.9403 - val_loss: 0.7114 - val_acc: 0.7778\n",
      "Epoch 2/60\n",
      "298/298 [==============================] - 104s 350ms/step - loss: 0.0065 - acc: 0.9990 - val_loss: 0.7147 - val_acc: 0.7987\n",
      "Epoch 3/60\n",
      "298/298 [==============================] - 104s 351ms/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.6159 - val_acc: 0.8242\n",
      "Epoch 4/60\n",
      "298/298 [==============================] - 104s 351ms/step - loss: 9.3733e-04 - acc: 0.9998 - val_loss: 0.5811 - val_acc: 0.8315\n",
      "Epoch 5/60\n",
      "298/298 [==============================] - 104s 351ms/step - loss: 6.2988e-04 - acc: 0.9999 - val_loss: 0.5754 - val_acc: 0.8363\n",
      "Epoch 6/60\n",
      "298/298 [==============================] - 104s 351ms/step - loss: 2.7533e-04 - acc: 1.0000 - val_loss: 0.5589 - val_acc: 0.8433\n",
      "Epoch 7/60\n",
      "298/298 [==============================] - 104s 351ms/step - loss: 2.3530e-04 - acc: 1.0000 - val_loss: 0.5694 - val_acc: 0.8433\n",
      "Epoch 8/60\n",
      "298/298 [==============================] - 105s 351ms/step - loss: 1.7003e-04 - acc: 1.0000 - val_loss: 0.5777 - val_acc: 0.8424\n",
      "Epoch 9/60\n",
      "298/298 [==============================] - 104s 351ms/step - loss: 1.6907e-04 - acc: 1.0000 - val_loss: 0.5936 - val_acc: 0.8439\n",
      "Epoch 10/60\n",
      "298/298 [==============================] - 105s 351ms/step - loss: 1.3069e-04 - acc: 1.0000 - val_loss: 0.6219 - val_acc: 0.8366\n",
      "Epoch 11/60\n",
      "298/298 [==============================] - 104s 351ms/step - loss: 1.3468e-04 - acc: 1.0000 - val_loss: 0.6061 - val_acc: 0.8412\n",
      "Found 79726 images belonging to 1 classes.\n",
      "79726\n"
     ]
    }
   ],
   "source": [
    "#lr adadelta 0.001,dr 0.5\n",
    "model = train_model_2(restNet_model, save_file='model_resnet.h5')\n",
    "test_model_2(model, 'pred.csv')\n",
    "data = pd.read_csv(\"pred.csv\")\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv2d_1\n",
      "2 batch_normalization_1\n",
      "3 activation_1\n",
      "4 conv2d_2\n",
      "5 batch_normalization_2\n",
      "6 activation_2\n",
      "7 conv2d_3\n",
      "8 batch_normalization_3\n",
      "9 activation_3\n",
      "10 max_pooling2d_1\n",
      "11 conv2d_4\n",
      "12 batch_normalization_4\n",
      "13 activation_4\n",
      "14 conv2d_5\n",
      "15 batch_normalization_5\n",
      "16 activation_5\n",
      "17 max_pooling2d_2\n",
      "18 conv2d_9\n",
      "19 batch_normalization_9\n",
      "20 activation_9\n",
      "21 conv2d_7\n",
      "22 conv2d_10\n",
      "23 batch_normalization_7\n",
      "24 batch_normalization_10\n",
      "25 activation_7\n",
      "26 activation_10\n",
      "27 average_pooling2d_1\n",
      "28 conv2d_6\n",
      "29 conv2d_8\n",
      "30 conv2d_11\n",
      "31 conv2d_12\n",
      "32 batch_normalization_6\n",
      "33 batch_normalization_8\n",
      "34 batch_normalization_11\n",
      "35 batch_normalization_12\n",
      "36 activation_6\n",
      "37 activation_8\n",
      "38 activation_11\n",
      "39 activation_12\n",
      "40 mixed0\n",
      "41 conv2d_16\n",
      "42 batch_normalization_16\n",
      "43 activation_16\n",
      "44 conv2d_14\n",
      "45 conv2d_17\n",
      "46 batch_normalization_14\n",
      "47 batch_normalization_17\n",
      "48 activation_14\n",
      "49 activation_17\n",
      "50 average_pooling2d_2\n",
      "51 conv2d_13\n",
      "52 conv2d_15\n",
      "53 conv2d_18\n",
      "54 conv2d_19\n",
      "55 batch_normalization_13\n",
      "56 batch_normalization_15\n",
      "57 batch_normalization_18\n",
      "58 batch_normalization_19\n",
      "59 activation_13\n",
      "60 activation_15\n",
      "61 activation_18\n",
      "62 activation_19\n",
      "63 mixed1\n",
      "64 conv2d_23\n",
      "65 batch_normalization_23\n",
      "66 activation_23\n",
      "67 conv2d_21\n",
      "68 conv2d_24\n",
      "69 batch_normalization_21\n",
      "70 batch_normalization_24\n",
      "71 activation_21\n",
      "72 activation_24\n",
      "73 average_pooling2d_3\n",
      "74 conv2d_20\n",
      "75 conv2d_22\n",
      "76 conv2d_25\n",
      "77 conv2d_26\n",
      "78 batch_normalization_20\n",
      "79 batch_normalization_22\n",
      "80 batch_normalization_25\n",
      "81 batch_normalization_26\n",
      "82 activation_20\n",
      "83 activation_22\n",
      "84 activation_25\n",
      "85 activation_26\n",
      "86 mixed2\n",
      "87 conv2d_28\n",
      "88 batch_normalization_28\n",
      "89 activation_28\n",
      "90 conv2d_29\n",
      "91 batch_normalization_29\n",
      "92 activation_29\n",
      "93 conv2d_27\n",
      "94 conv2d_30\n",
      "95 batch_normalization_27\n",
      "96 batch_normalization_30\n",
      "97 activation_27\n",
      "98 activation_30\n",
      "99 max_pooling2d_3\n",
      "100 mixed3\n",
      "101 conv2d_35\n",
      "102 batch_normalization_35\n",
      "103 activation_35\n",
      "104 conv2d_36\n",
      "105 batch_normalization_36\n",
      "106 activation_36\n",
      "107 conv2d_32\n",
      "108 conv2d_37\n",
      "109 batch_normalization_32\n",
      "110 batch_normalization_37\n",
      "111 activation_32\n",
      "112 activation_37\n",
      "113 conv2d_33\n",
      "114 conv2d_38\n",
      "115 batch_normalization_33\n",
      "116 batch_normalization_38\n",
      "117 activation_33\n",
      "118 activation_38\n",
      "119 average_pooling2d_4\n",
      "120 conv2d_31\n",
      "121 conv2d_34\n",
      "122 conv2d_39\n",
      "123 conv2d_40\n",
      "124 batch_normalization_31\n",
      "125 batch_normalization_34\n",
      "126 batch_normalization_39\n",
      "127 batch_normalization_40\n",
      "128 activation_31\n",
      "129 activation_34\n",
      "130 activation_39\n",
      "131 activation_40\n",
      "132 mixed4\n",
      "133 conv2d_45\n",
      "134 batch_normalization_45\n",
      "135 activation_45\n",
      "136 conv2d_46\n",
      "137 batch_normalization_46\n",
      "138 activation_46\n",
      "139 conv2d_42\n",
      "140 conv2d_47\n",
      "141 batch_normalization_42\n",
      "142 batch_normalization_47\n",
      "143 activation_42\n",
      "144 activation_47\n",
      "145 conv2d_43\n",
      "146 conv2d_48\n",
      "147 batch_normalization_43\n",
      "148 batch_normalization_48\n",
      "149 activation_43\n",
      "150 activation_48\n",
      "151 average_pooling2d_5\n",
      "152 conv2d_41\n",
      "153 conv2d_44\n",
      "154 conv2d_49\n",
      "155 conv2d_50\n",
      "156 batch_normalization_41\n",
      "157 batch_normalization_44\n",
      "158 batch_normalization_49\n",
      "159 batch_normalization_50\n",
      "160 activation_41\n",
      "161 activation_44\n",
      "162 activation_49\n",
      "163 activation_50\n",
      "164 mixed5\n",
      "165 conv2d_55\n",
      "166 batch_normalization_55\n",
      "167 activation_55\n",
      "168 conv2d_56\n",
      "169 batch_normalization_56\n",
      "170 activation_56\n",
      "171 conv2d_52\n",
      "172 conv2d_57\n",
      "173 batch_normalization_52\n",
      "174 batch_normalization_57\n",
      "175 activation_52\n",
      "176 activation_57\n",
      "177 conv2d_53\n",
      "178 conv2d_58\n",
      "179 batch_normalization_53\n",
      "180 batch_normalization_58\n",
      "181 activation_53\n",
      "182 activation_58\n",
      "183 average_pooling2d_6\n",
      "184 conv2d_51\n",
      "185 conv2d_54\n",
      "186 conv2d_59\n",
      "187 conv2d_60\n",
      "188 batch_normalization_51\n",
      "189 batch_normalization_54\n",
      "190 batch_normalization_59\n",
      "191 batch_normalization_60\n",
      "192 activation_51\n",
      "193 activation_54\n",
      "194 activation_59\n",
      "195 activation_60\n",
      "196 mixed6\n",
      "197 conv2d_65\n",
      "198 batch_normalization_65\n",
      "199 activation_65\n",
      "200 conv2d_66\n",
      "201 batch_normalization_66\n",
      "202 activation_66\n",
      "203 conv2d_62\n",
      "204 conv2d_67\n",
      "205 batch_normalization_62\n",
      "206 batch_normalization_67\n",
      "207 activation_62\n",
      "208 activation_67\n",
      "209 conv2d_63\n",
      "210 conv2d_68\n",
      "211 batch_normalization_63\n",
      "212 batch_normalization_68\n",
      "213 activation_63\n",
      "214 activation_68\n",
      "215 average_pooling2d_7\n",
      "216 conv2d_61\n",
      "217 conv2d_64\n",
      "218 conv2d_69\n",
      "219 conv2d_70\n",
      "220 batch_normalization_61\n",
      "221 batch_normalization_64\n",
      "222 batch_normalization_69\n",
      "223 batch_normalization_70\n",
      "224 activation_61\n",
      "225 activation_64\n",
      "226 activation_69\n",
      "227 activation_70\n",
      "228 mixed7\n",
      "229 conv2d_73\n",
      "230 batch_normalization_73\n",
      "231 activation_73\n",
      "232 conv2d_74\n",
      "233 batch_normalization_74\n",
      "234 activation_74\n",
      "235 conv2d_71\n",
      "236 conv2d_75\n",
      "237 batch_normalization_71\n",
      "238 batch_normalization_75\n",
      "239 activation_71\n",
      "240 activation_75\n",
      "241 conv2d_72\n",
      "242 conv2d_76\n",
      "243 batch_normalization_72\n",
      "244 batch_normalization_76\n",
      "245 activation_72\n",
      "246 activation_76\n",
      "247 max_pooling2d_4\n",
      "248 mixed8\n",
      "249 conv2d_81\n",
      "250 batch_normalization_81\n",
      "251 activation_81\n",
      "252 conv2d_78\n",
      "253 conv2d_82\n",
      "254 batch_normalization_78\n",
      "255 batch_normalization_82\n",
      "256 activation_78\n",
      "257 activation_82\n",
      "258 conv2d_79\n",
      "259 conv2d_80\n",
      "260 conv2d_83\n",
      "261 conv2d_84\n",
      "262 average_pooling2d_8\n",
      "263 conv2d_77\n",
      "264 batch_normalization_79\n",
      "265 batch_normalization_80\n",
      "266 batch_normalization_83\n",
      "267 batch_normalization_84\n",
      "268 conv2d_85\n",
      "269 batch_normalization_77\n",
      "270 activation_79\n",
      "271 activation_80\n",
      "272 activation_83\n",
      "273 activation_84\n",
      "274 batch_normalization_85\n",
      "275 activation_77\n",
      "276 mixed9_0\n",
      "277 concatenate_1\n",
      "278 activation_85\n",
      "279 mixed9\n",
      "280 conv2d_90\n",
      "281 batch_normalization_90\n",
      "282 activation_90\n",
      "283 conv2d_87\n",
      "284 conv2d_91\n",
      "285 batch_normalization_87\n",
      "286 batch_normalization_91\n",
      "287 activation_87\n",
      "288 activation_91\n",
      "289 conv2d_88\n",
      "290 conv2d_89\n",
      "291 conv2d_92\n",
      "292 conv2d_93\n",
      "293 average_pooling2d_9\n",
      "294 conv2d_86\n",
      "295 batch_normalization_88\n",
      "296 batch_normalization_89\n",
      "297 batch_normalization_92\n",
      "298 batch_normalization_93\n",
      "299 conv2d_94\n",
      "300 batch_normalization_86\n",
      "301 activation_88\n",
      "302 activation_89\n",
      "303 activation_92\n",
      "304 activation_93\n",
      "305 batch_normalization_94\n",
      "306 activation_86\n",
      "307 mixed9_1\n",
      "308 concatenate_2\n",
      "309 activation_94\n",
      "310 mixed10\n",
      "p002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:25: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p012\n",
      "p014\n",
      "p015\n",
      "Epoch 1/60\n",
      "298/298 [==============================] - 315s 1s/step - loss: 0.6125 - acc: 0.8849 - val_loss: 0.9169 - val_acc: 0.7263\n",
      "Epoch 2/60\n",
      "298/298 [==============================] - 120s 403ms/step - loss: 0.0276 - acc: 0.9977 - val_loss: 1.1091 - val_acc: 0.6732\n",
      "Epoch 3/60\n",
      "298/298 [==============================] - 120s 403ms/step - loss: 0.0133 - acc: 0.9984 - val_loss: 1.0108 - val_acc: 0.7081\n",
      "Epoch 4/60\n",
      "298/298 [==============================] - 120s 402ms/step - loss: 0.0069 - acc: 0.9993 - val_loss: 0.9484 - val_acc: 0.7436\n",
      "Epoch 5/60\n",
      " 46/298 [===>..........................] - ETA: 1:34 - loss: 0.0033 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "\n",
    "#lr adam 0.00005,dr 0.5 from layer11,229,113,51,5,9,14\n",
    "model = train_model_2(inceptionV3_model, save_file='model_inceptionV3.h5')\n",
    "test_model_2(model, 'pred.csv')\n",
    "data = pd.read_csv(\"pred.csv\")\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv2d_1\n",
      "2 batch_normalization_1\n",
      "3 activation_1\n",
      "4 conv2d_2\n",
      "5 batch_normalization_2\n",
      "6 activation_2\n",
      "7 conv2d_3\n",
      "8 batch_normalization_3\n",
      "9 activation_3\n",
      "10 max_pooling2d_1\n",
      "11 conv2d_4\n",
      "12 batch_normalization_4\n",
      "13 activation_4\n",
      "14 conv2d_5\n",
      "15 batch_normalization_5\n",
      "16 activation_5\n",
      "17 max_pooling2d_2\n",
      "18 conv2d_9\n",
      "19 batch_normalization_9\n",
      "20 activation_9\n",
      "21 conv2d_7\n",
      "22 conv2d_10\n",
      "23 batch_normalization_7\n",
      "24 batch_normalization_10\n",
      "25 activation_7\n",
      "26 activation_10\n",
      "27 average_pooling2d_1\n",
      "28 conv2d_6\n",
      "29 conv2d_8\n",
      "30 conv2d_11\n",
      "31 conv2d_12\n",
      "32 batch_normalization_6\n",
      "33 batch_normalization_8\n",
      "34 batch_normalization_11\n",
      "35 batch_normalization_12\n",
      "36 activation_6\n",
      "37 activation_8\n",
      "38 activation_11\n",
      "39 activation_12\n",
      "40 mixed0\n",
      "41 conv2d_16\n",
      "42 batch_normalization_16\n",
      "43 activation_16\n",
      "44 conv2d_14\n",
      "45 conv2d_17\n",
      "46 batch_normalization_14\n",
      "47 batch_normalization_17\n",
      "48 activation_14\n",
      "49 activation_17\n",
      "50 average_pooling2d_2\n",
      "51 conv2d_13\n",
      "52 conv2d_15\n",
      "53 conv2d_18\n",
      "54 conv2d_19\n",
      "55 batch_normalization_13\n",
      "56 batch_normalization_15\n",
      "57 batch_normalization_18\n",
      "58 batch_normalization_19\n",
      "59 activation_13\n",
      "60 activation_15\n",
      "61 activation_18\n",
      "62 activation_19\n",
      "63 mixed1\n",
      "64 conv2d_23\n",
      "65 batch_normalization_23\n",
      "66 activation_23\n",
      "67 conv2d_21\n",
      "68 conv2d_24\n",
      "69 batch_normalization_21\n",
      "70 batch_normalization_24\n",
      "71 activation_21\n",
      "72 activation_24\n",
      "73 average_pooling2d_3\n",
      "74 conv2d_20\n",
      "75 conv2d_22\n",
      "76 conv2d_25\n",
      "77 conv2d_26\n",
      "78 batch_normalization_20\n",
      "79 batch_normalization_22\n",
      "80 batch_normalization_25\n",
      "81 batch_normalization_26\n",
      "82 activation_20\n",
      "83 activation_22\n",
      "84 activation_25\n",
      "85 activation_26\n",
      "86 mixed2\n",
      "87 conv2d_28\n",
      "88 batch_normalization_28\n",
      "89 activation_28\n",
      "90 conv2d_29\n",
      "91 batch_normalization_29\n",
      "92 activation_29\n",
      "93 conv2d_27\n",
      "94 conv2d_30\n",
      "95 batch_normalization_27\n",
      "96 batch_normalization_30\n",
      "97 activation_27\n",
      "98 activation_30\n",
      "99 max_pooling2d_3\n",
      "100 mixed3\n",
      "101 conv2d_35\n",
      "102 batch_normalization_35\n",
      "103 activation_35\n",
      "104 conv2d_36\n",
      "105 batch_normalization_36\n",
      "106 activation_36\n",
      "107 conv2d_32\n",
      "108 conv2d_37\n",
      "109 batch_normalization_32\n",
      "110 batch_normalization_37\n",
      "111 activation_32\n",
      "112 activation_37\n",
      "113 conv2d_33\n",
      "114 conv2d_38\n",
      "115 batch_normalization_33\n",
      "116 batch_normalization_38\n",
      "117 activation_33\n",
      "118 activation_38\n",
      "119 average_pooling2d_4\n",
      "120 conv2d_31\n",
      "121 conv2d_34\n",
      "122 conv2d_39\n",
      "123 conv2d_40\n",
      "124 batch_normalization_31\n",
      "125 batch_normalization_34\n",
      "126 batch_normalization_39\n",
      "127 batch_normalization_40\n",
      "128 activation_31\n",
      "129 activation_34\n",
      "130 activation_39\n",
      "131 activation_40\n",
      "132 mixed4\n",
      "133 conv2d_45\n",
      "134 batch_normalization_45\n",
      "135 activation_45\n",
      "136 conv2d_46\n",
      "137 batch_normalization_46\n",
      "138 activation_46\n",
      "139 conv2d_42\n",
      "140 conv2d_47\n",
      "141 batch_normalization_42\n",
      "142 batch_normalization_47\n",
      "143 activation_42\n",
      "144 activation_47\n",
      "145 conv2d_43\n",
      "146 conv2d_48\n",
      "147 batch_normalization_43\n",
      "148 batch_normalization_48\n",
      "149 activation_43\n",
      "150 activation_48\n",
      "151 average_pooling2d_5\n",
      "152 conv2d_41\n",
      "153 conv2d_44\n",
      "154 conv2d_49\n",
      "155 conv2d_50\n",
      "156 batch_normalization_41\n",
      "157 batch_normalization_44\n",
      "158 batch_normalization_49\n",
      "159 batch_normalization_50\n",
      "160 activation_41\n",
      "161 activation_44\n",
      "162 activation_49\n",
      "163 activation_50\n",
      "164 mixed5\n",
      "165 conv2d_55\n",
      "166 batch_normalization_55\n",
      "167 activation_55\n",
      "168 conv2d_56\n",
      "169 batch_normalization_56\n",
      "170 activation_56\n",
      "171 conv2d_52\n",
      "172 conv2d_57\n",
      "173 batch_normalization_52\n",
      "174 batch_normalization_57\n",
      "175 activation_52\n",
      "176 activation_57\n",
      "177 conv2d_53\n",
      "178 conv2d_58\n",
      "179 batch_normalization_53\n",
      "180 batch_normalization_58\n",
      "181 activation_53\n",
      "182 activation_58\n",
      "183 average_pooling2d_6\n",
      "184 conv2d_51\n",
      "185 conv2d_54\n",
      "186 conv2d_59\n",
      "187 conv2d_60\n",
      "188 batch_normalization_51\n",
      "189 batch_normalization_54\n",
      "190 batch_normalization_59\n",
      "191 batch_normalization_60\n",
      "192 activation_51\n",
      "193 activation_54\n",
      "194 activation_59\n",
      "195 activation_60\n",
      "196 mixed6\n",
      "197 conv2d_65\n",
      "198 batch_normalization_65\n",
      "199 activation_65\n",
      "200 conv2d_66\n",
      "201 batch_normalization_66\n",
      "202 activation_66\n",
      "203 conv2d_62\n",
      "204 conv2d_67\n",
      "205 batch_normalization_62\n",
      "206 batch_normalization_67\n",
      "207 activation_62\n",
      "208 activation_67\n",
      "209 conv2d_63\n",
      "210 conv2d_68\n",
      "211 batch_normalization_63\n",
      "212 batch_normalization_68\n",
      "213 activation_63\n",
      "214 activation_68\n",
      "215 average_pooling2d_7\n",
      "216 conv2d_61\n",
      "217 conv2d_64\n",
      "218 conv2d_69\n",
      "219 conv2d_70\n",
      "220 batch_normalization_61\n",
      "221 batch_normalization_64\n",
      "222 batch_normalization_69\n",
      "223 batch_normalization_70\n",
      "224 activation_61\n",
      "225 activation_64\n",
      "226 activation_69\n",
      "227 activation_70\n",
      "228 mixed7\n",
      "229 conv2d_73\n",
      "230 batch_normalization_73\n",
      "231 activation_73\n",
      "232 conv2d_74\n",
      "233 batch_normalization_74\n",
      "234 activation_74\n",
      "235 conv2d_71\n",
      "236 conv2d_75\n",
      "237 batch_normalization_71\n",
      "238 batch_normalization_75\n",
      "239 activation_71\n",
      "240 activation_75\n",
      "241 conv2d_72\n",
      "242 conv2d_76\n",
      "243 batch_normalization_72\n",
      "244 batch_normalization_76\n",
      "245 activation_72\n",
      "246 activation_76\n",
      "247 max_pooling2d_4\n",
      "248 mixed8\n",
      "249 conv2d_81\n",
      "250 batch_normalization_81\n",
      "251 activation_81\n",
      "252 conv2d_78\n",
      "253 conv2d_82\n",
      "254 batch_normalization_78\n",
      "255 batch_normalization_82\n",
      "256 activation_78\n",
      "257 activation_82\n",
      "258 conv2d_79\n",
      "259 conv2d_80\n",
      "260 conv2d_83\n",
      "261 conv2d_84\n",
      "262 average_pooling2d_8\n",
      "263 conv2d_77\n",
      "264 batch_normalization_79\n",
      "265 batch_normalization_80\n",
      "266 batch_normalization_83\n",
      "267 batch_normalization_84\n",
      "268 conv2d_85\n",
      "269 batch_normalization_77\n",
      "270 activation_79\n",
      "271 activation_80\n",
      "272 activation_83\n",
      "273 activation_84\n",
      "274 batch_normalization_85\n",
      "275 activation_77\n",
      "276 mixed9_0\n",
      "277 concatenate_1\n",
      "278 activation_85\n",
      "279 mixed9\n",
      "280 conv2d_90\n",
      "281 batch_normalization_90\n",
      "282 activation_90\n",
      "283 conv2d_87\n",
      "284 conv2d_91\n",
      "285 batch_normalization_87\n",
      "286 batch_normalization_91\n",
      "287 activation_87\n",
      "288 activation_91\n",
      "289 conv2d_88\n",
      "290 conv2d_89\n",
      "291 conv2d_92\n",
      "292 conv2d_93\n",
      "293 average_pooling2d_9\n",
      "294 conv2d_86\n",
      "295 batch_normalization_88\n",
      "296 batch_normalization_89\n",
      "297 batch_normalization_92\n",
      "298 batch_normalization_93\n",
      "299 conv2d_94\n",
      "300 batch_normalization_86\n",
      "301 activation_88\n",
      "302 activation_89\n",
      "303 activation_92\n",
      "304 activation_93\n",
      "305 batch_normalization_94\n",
      "306 activation_86\n",
      "307 mixed9_1\n",
      "308 concatenate_2\n",
      "309 activation_94\n",
      "310 mixed10\n",
      "p002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:27: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p012\n",
      "p014\n",
      "p015\n",
      "Epoch 1/60\n",
      "298/298 [==============================] - 348s 1s/step - loss: 0.3464 - acc: 0.9257 - val_loss: 1.2708 - val_acc: 0.6223\n",
      "Epoch 2/60\n",
      "298/298 [==============================] - 141s 474ms/step - loss: 0.0123 - acc: 0.9979 - val_loss: 0.8702 - val_acc: 0.7351\n",
      "Epoch 3/60\n",
      "298/298 [==============================] - 141s 474ms/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.9187 - val_acc: 0.7436\n",
      "Epoch 4/60\n",
      "298/298 [==============================] - 141s 474ms/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.9002 - val_acc: 0.7472\n",
      "Epoch 5/60\n",
      "188/298 [=================>............] - ETA: 49s - loss: 8.8382e-04 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "\n",
    "#lr sgd 0.0005,dr 0.5 \n",
    "model = train_model_2(inception_resnet_v2_model, save_file='model_inceptionRes.h5')\n",
    "test_model_2(model, 'pred.csv')\n",
    "data = pd.read_csv(\"pred.csv\")\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义CAM可视化方法\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#from keras.applications.vgg19 import preprocess_input\n",
    "#from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "def cam(model):\n",
    "    print(np.array(model.layers[173]).shape)\n",
    "    weights = model.layers[173].get_weights()[0]\n",
    "    model2 = Model(model.input, [model.layers[172].output, model.output])\n",
    "        \n",
    "    test_datagen = ImageDataGenerator(\n",
    " #   shear_range=0.2,\n",
    " #   zoom_range=0.2,\n",
    "    preprocessing_function=preprocess_input,\n",
    " #   horizontal_flip=True\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        'imgs_2/data',\n",
    "        target_size=(image_size[0], image_size[1]),\n",
    "        shuffle = False,\n",
    "        class_mode='categorical',\n",
    "        batch_size=1)\n",
    "\n",
    "    out, prediction = model2.predict_generator(test_generator, steps = 15)\n",
    "    print(prediction.shape)\n",
    " #   prediction = prediction[0]\n",
    "    print(out.shape)\n",
    " #   out = out[0]\n",
    "\n",
    "    for i, pre in enumerate(prediction) :\n",
    "     #   plt.title('steer %.2f' % (prediction))\n",
    "        cam = np.matmul(out[i], weights) \n",
    "        print(cam.shape)\n",
    "        print(prediction[i].shape)\n",
    "        cam = (prediction[i] - 0.0)  * cam      \n",
    "        cam -= cam.min()\n",
    "        cam /= cam.max()\n",
    "        cam -= 0.2\n",
    "    \n",
    "        print(cam.shape)\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "        heatmap[np.where(cam <= 0.2)] = 0\n",
    "        \n",
    "\n",
    "        out = cv2.addWeighted(img, 0.8, heatmap, 0.4, 0)\n",
    "        plt.imshow(out[:,:,::-1])\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "Found 79726 images belonging to 1 classes.\n",
      "(15, 10)\n",
      "(15, 7, 7, 2048)\n",
      "(7, 7, 10)\n",
      "(10,)\n",
      "(7, 7, 10)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "/tmp/build/80754af9/opencv_1512680316562/work/modules/imgproc/src/colormap.cpp:517: error: (-5) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function operator()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-a1ef488e4c43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-e2257e2ca25e>\u001b[0m in \u001b[0;36mcam\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyColorMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLORMAP_JET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mheatmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /tmp/build/80754af9/opencv_1512680316562/work/modules/imgproc/src/colormap.cpp:517: error: (-5) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function operator()\n"
     ]
    }
   ],
   "source": [
    "cam(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
